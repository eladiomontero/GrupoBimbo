missClass(testSA$chd, predict(model, testSA))
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
vowel.train
vowel.test
View(vowel.test)
vowel.test$y = as.factor(vowel.test$y)
vowel.train$y = as.factor(vowel.train$y)
set.seed(33833)
model = train(y~., data = vowel.train, method = "rf")
?importane
?importance
?varImp
varImp(model)
class(vowel.train$y)
vowel.train$y
varImp(model, value= y)
model = train(y~., data = vowel.train, method = "rf", importance = T)
varImp(model, value= y)
varImp(model)
model = randomForest(y~., data = vowel.train)
varImp(model)
sort(t(varImp(fit2)),index=TRUE,decreasing=TRUE)
sort(t(varImp(model)),index=TRUE,decreasing=TRUE)
model = randomForest(y~., data = vowel.train, importance = T)
sort(t(varImp(model)),index=TRUE,decreasing=TRUE)
order(model$importance[,13],decreasing=T)
library(AppliedPredictiveModeling)
data(segmentationOriginal)
library(caret)
inTrain = createDataPartition(y=segmentationOriginal$Case, p = 0.75, list = F)
training = segmentationOriginal[inTrain,]
testing = segmentationOriginal[-inTrain,]
set.seed(125)
model = train(Case~., data = training, method = "rpart")
model = rpart(Class~., data = training)
fancyRpartPlot(model$finalModel)
fancyRpartPlot(model)
47e+3
library(AppliedPredictiveModeling)
data(segmentationOriginal)
library(caret)
inTrain = createDataPartition(y=segmentationOriginal$Case, p = 0.75, list = F)
training = subset(segmentationOriginal, Case = "Train")
model = train(Case~., data = training, method = "rpart")
fancyRpartPlot(model)
library(AppliedPredictiveModeling)
data(segmentationOriginal)
library(caret)
inTrain = createDataPartition(y=segmentationOriginal$Case, p = 0.75, list = F)
training = subset(segmentationOriginal, Case = "Train")
model = rpart(Case~., data = training)
fancyRpartPlot(model)
library(AppliedPredictiveModeling)
data(segmentationOriginal)
library(caret)
inTrain = createDataPartition(y=segmentationOriginal$Case, p = 0.75, list = F)
training = subset(segmentationOriginal, Case = "Train")
model = rpart(Class~., data = training)
fancyRpartPlot(model)
model = train(chd~alcohol+obesity+tobacco+typea+ldl, data = trainSA, method = "glm", family = "binomial")
missClass(trainSA$chd, predict(model, trainSA))
missClass(testSA$chd, predict(model, testSA))
library(AppliedPredictiveModeling)
data(segmentationOriginal)
library(caret)
inTrain = createDataPartition(y=segmentationOriginal$Case, p = 0.75, list = F)
training = subset(segmentationOriginal, Case = "Train")
model = train(Class~., data = training, method = "rpart")
model
model$finalMode
model$finalModel
plot(model$finalModel)
fancyRpartPlot(model$finalModel)
47e+3
plot(modFit$finalModel, uniform = TRUE, main = "Classification Tree")
plot(model$finalModel, uniform = TRUE, main = "Classification Tree")
text(model$finalModel, use.n = TRUE, all = TRUE, cex = .8)
set.seed(125)
inTrain <- createDataPartition(y = segmentationOriginal$Case, list = FALSE)
train <- subset(segmentationOriginal, Case == "Train")
test <- subset(segmentationOriginal, Case == "Test")
modFit <- train(Class ~ ., data = train, method = "rpart")
modFit$finalModel
fancyRpartPlot(modFit$finalModel)
library(AppliedPredictiveModeling)
data(segmentationOriginal)
library(caret)
inTrain = createDataPartition(y=segmentationOriginal$Case, p = 0.75, list = F)
training = subset(segmentationOriginal, Case = "Train")
set.seed(125)
model = train(Class~., data = training, method = "rpart")
fancyRpartPlot(model$finalModel)
training = subset(segmentationOriginal, Case == "Train")
testing = segmentationOriginal[-inTrain,]
set.seed(125)
model = train(Case~., data = training, method = "rpart")
training = subset(segmentationOriginal, Case == "Train")
set.seed(125)
model = train(Case~., data = training, method = "rpart")
training = subset(segmentationOriginal, Case == "Train")
set.seed(125)
model = train(Class~., data = training, method = "rpart")
fancyRpartPlot(model$finalModel)
training = subset(segmentationOriginal, Case = "Train")
model = train(chd~age+alcohol+obesity+tobacco+typea+ldl, data = trainSA, method = "glm", family = "binomial")
missClass(testSA$chd, predict(model, testSA))
missClass(trainSA$chd, predict(model, trainSA))
library(manipulate)
manipulate(plot(1:x), x = slider(1,
100))
library(manipulate)
myHist <- function(mu){
hist(galton$child,col="blue",breaks=100)
lines(c(mu, mu), c(0, 150),col="red",lwd=5)
mse <- mean((galton$child - mu)^2)
text(63, 150, paste("mu = ", mu))
text(63, 140, paste("MSE = ", round(mse, 2)))
}
manipulate(myHist(mu), mu = slider(62, 74, step = 0.5))
library(manipulate)
myPlot <- function(s) {
plot(cars$dist - mean(cars$dist), cars$speed - mean(cars$speed))
abline(0, s)
}
manipulate(myPlot, s = slider(0, 2, step = 0.1))
myPlot
myPlot()
manipulate(myPlot(s), slider = x(0, 2, step = 0.1))
manipulate(myPlot(s), x.s = slider(0, 2, step = 0.1))
manipulate(myPlot(s), s = slider(0, 2, step = 0.1))
library(rCharts)
require(rCharts)
require("rCharts")
install.packages("rCharts")
require(rCharts)
test <- read.csv("C:/Users/eladiomontero/Desktop/SF Crime/test.csv")
View(test)
library(ggplot2)
library(caret)
train <- read.csv("C:/Users/eladiomontero/Desktop/SF Crime/train.csv")
View(train)
inTrain = createDataPartition(y = train$Category, p = 0.7, list = F)
train_crime = train[inTrain,]
test_crime = train[-inTrain,]
featurePlot(x= train_crime[, c("Descript", "DayOfWeek","PdDistrict")], y = train_crime$Category, plot = "pairs")
summary(train_crime$Category)
hist(train_crime$Category)
plot(train_crime$Category)
plot(train_crime$Category, type = "bar")
?plot
library(lubridate)
summary(train_crime$Dates)
class(train_crime$Dates)
d = as.Date(train_crime$Dates)
summary(d)
?as.Date
install.packages("reshape2")
j = colsplit(train_crime$Dates, " ", c("date", "hour"))
library(reshape2)
j = colsplit(train_crime$Dates, " ", c("date", "hour"))
View(test)
View(j)
train_crime = cbind(train_crime, j)
summary(train_crime$hour)
summary(train_crime$date)
train_crime$date = as.Date(train_crime$date)
summary(train_crime$date)
View(train_crime)
h = strptime(train_crime$hour, format = "%H:%M:%S", tz = ""))
h = strptime(train_crime$hour, format = "%H:%M:%S", tz = "")
summary(h)
hour("2015-06-22 13:45:11")
h = hour(h)
h
train_crime$hour = h
View(train_crime)
ggplot(train_crime, aes(factor(Category), hour)) + geom_boxplot()
library(ggplot2)
ggplot(train_crime, aes(factor(Category), hour)) + geom_boxplot()
length(unique(train_crime$Category))
plot(train_crime$weekday)
plot(train_crime$DayOfWeek)
plot(train_crime$hour)
summary(train_crime$hour)
unique(train_crime$hour)
plot(factor(train_crime$hour))
save.image("C:/Users/eladiomontero/Desktop/SF Crime/sf_crime.RData")
librry(tm)
library(tm)
library(RWeka)
file_input = readLines(file.choose(), n = 5000)
corpus_text = VectorSource(file_input)
corpus = Corpus(corpus_text)
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, stripWhitespace)
corpus = tm_map(corpus, content_transformer(remove_special))
dtm <- DocumentTermMatrix(corpus)
?DocumentTermMatrix
inspect(dtm["blog"])
inspect(dtm[c("blog")])
inspect(dtm[c("blog"), 1])
inspect(dtm[c("blog"), 2])
inspect(dtm[c("blog"), c("1")])
inspect(dtm[1:5, 273:276])
TrigramTokenizer <- function(x) NGramTokenizer(x,
Weka_control(min = 3, max = 3))
tdm <- TermDocumentMatrix(corpus, control = list(tokenize = TrigramTokenizer))
inspect(tdm[202:205, 1:5])
inspect(tdm[202, 1:5])
inspect(tdm["blog", 1:5])
inspect(tdm["5000", 1:5])
inspect(tdm["6", 1:5])
inspect(tdm[6, 1:5])
inspect(tdm[7, 1:5])
inspect(removeSparseTerms(tdm[, 1:10], 0.7))
inspect(removeSparseTerms(tdm[1000:1001, 1:10], 0.7))
inspect(tdm[1000:1001, 1:10], 0.7)
inspect(tdm[1000:1001, 1:100])
inspect(tdm[1000:1004, 1:10], 0.7)
inspect(tdm[1000:1004, 1:10])
tdm <- TermDocumentMatrix(corpus, control = list(tokenize = sentDetect))
library(openNLP)
install.packages("openNLP")
library(openNLP)
tdm <- TermDocumentMatrix(corpus, control = list(tokenize = sentDetect))
s <- "This is a sentence. This another---but with dash-like structures, and some commas.
Maybe another with question marks? Sure!"
sentDetect(s, language = "en")
library(openNLP)
?sentDetect
??sentDetect
sentDetect
tokenize
?openNLP
require(openNLP)
openNLP
?openNLP
NLP
library(NLP)
NLP
?NLP
Maxent_Sent_Token_Annotator()
tdm <- TermDocumentMatrix(corpus, control = list(tokenize = Maxent_Sent_Token_Annotator()))
inspect(tdm[1000:1004, 1:10], 0.7)
inspect(tdm[1000:1004, 1:10])
inspect(tdm[1, 1:10])
inspect(tdm[1;6, 1:10])
inspect(tdm[1:6, 1:10])
inspect(tdm[1:7, 1:10])
s <- "This is a sentence. This another---but with dash-like structures, and some commas.
Maybe another with question marks? Sure!"
Maxent_Sent_Token_Annotator(s, language = "en")
library("qdap")
install.packages(qdap)
install.packages("qdap")
install.packages("stylo")
library(stylo)
txt.to.words("And now, Laertes, what's the news with you?")
tagged.text = "The_DT family_NN of_IN Dashwood_NNP had_VBD long_RB
been_VBN settled_VBN in_IN Sussex_NNP ._."
txt.to.words(tagged.text, splitting.rule = "([A-Za-z,.;!]+_)|[ \n\t]")
tagged.text = "The_DT family_NN of_IN Dashwood_NNP had_VBD long_RB
been_VBN settled_VBN in_IN Sussex_NNP ._."
txt.to.words(tagged.text)
tagged.text = "The_DT family_NN of_IN Dashwood_NNP had_VBD long_RB
been_VBN settled_VBN in_IN Sussex_NNP ._."
txt.to.words(tagged.text, splitting.rule = "([A-Za-z,.;!+_])|[ \n\t]")
tagged.text = "The_DT family_NN of_IN Dashwood_NNP had_VBD long_RB
been_VBN settled_VBN in_IN Sussex_NNP ._."
txt.to.words(tagged.text, splitting.rule = "([A-Za-z,.;!])|[ \n\t]")
tagged.text = "The_DT family_NN of_IN Dashwood_NNP had_VBD long_RB
been_VBN settled_VBN in_IN Sussex_NNP ._."
txt.to.words(tagged.text, splitting.rule = "([A-Za-z,.;!]+ )|[ \n\t]")
tagged.text = "The_DT family_NN of_IN Dashwood_NNP had_VBD long_RB
been_VBN settled_VBN in_IN Sussex_NNP ._."
txt.to.words(tagged.text, splitting.rule = "([A-Za-z,.;!]+_ )|[ \n\t]")
tagged.text = "The_DT family_NN of_IN Dashwood_NNP had_VBD long_RB
been_VBN settled_VBN in_IN Sussex_NNP ._."
txt.to.words(tagged.text, splitting.rule = "([A-Za-z,.;!]+ _)|[ \n\t]")
tagged.text = "The_DT family_NN of_IN Dashwood_NNP had_VBD long_RB
been_VBN settled_VBN in_IN Sussex_NNP ._."
txt.to.words(tagged.text, splitting.rule = "[ \n\t]")
install.packages("RTextTools")
doc_matrix = create_matrix(file_input, language = "english", removeNumbers = T, stemWords= T, removeSparseTerms = 0.998)
library(RTextTools)
doc_matrix = create_matrix(file_input, language = "english", removeNumbers = T, stemWords= T, removeSparseTerms = 0.998)
inspect(doc_matrix)
dtm2 <- as.matrix(dtm)
freq <- colSums(as.matrix(dtm))
frequency <- sort(freq, decreasing=TRUE)
frequency
write.csv(dtm2, "matrix.csv", row.names = F)
t = numeric()
t
t = c(a=10, b=11)
t
y = c(a=10, b=11)
merge(t, y, on= row.names)
append(t, y)
y = c(a=10, b=11, c= 19)
t+y
sum(t+y)
c(t,y)
t = data.frame(t)
View(t)
y = data.frame(y)
t+y
View(t)
temp <- cbind(t,y)
temp <- rbind(t,y)
g = data.frame(colnames = c("caca", "us"))
View(g)
t = numeric(a = 10, b = 11, c = 13)
t = numeric(c(a = 10, b = 11, c = 13))
t = numeric()
t = c(a=10, b=11)
t
g = data.frame(freq = t)
View(g)
y = numeric()
y = c(a = 10, b = 45, c = 87, d = 43)
g$freq = g$freq + y
h = data.frame(freq = y)
plyr
library(plyr)
pp <- cbind(names=c(rownames(g), rownames(h),
rbind.fill(list(g, h)))
)
ddply(pp, .(names), function(x) colSums(x[,-1], na.rm = TRUE))
pp <- cbind(names=c(rownames(t), rownames(y),
rbind.fill(list(g, h))))
ddply(pp, .(names), function(x) colSums(x[,-1], na.rm = TRUE))
pp
pp <- cbind(names=c(rownames(t), rownames(y),
rbind.fill(list(t, y))))
pp <- cbind(names=c(rownames(g), rownames(h),
rbind.fill(list(g, h))))
pp
r = append(g,h)
r
r = rbind(g,h)
r
r = cbind(g,h)
?rbind
?merge
ddply(merge(g, h, all.x=TRUE),
.(row.names), summarise, freq=sum(freq))
g$word = row.names
g$word = g$row.names
View(g)
g
t$row.names
t[row.names]
library(data.table)
sapply(t, min)
sapply(t, sum)
sapply(c(t, y), sum)
install.packages("xlsx")
?xlsx
?read.xlsx
?write.xlsx
library(xls)
library(xlsx)
?read.xlsx
t = read.xlsx(file.choose(), sheetIndex = 1)
View(t)
library(transform)
install.packages("transform")
library(jsonlite)
t = read.xlsx(file.choose(), sheetIndex = 1)
View(t)
url = "https://maps.googleapis.com/maps/api/distancematrix/json?origins=San%20Ramon+Alajuela&destinations=Palmares+Alajuela&mode=driving"
document <- fromJSON(txt=url)
seg = document$rows$elements[[1]]$duration$value
library(curl)
install.packages("curl")
library(curl)
document <- fromJSON(txt=url)
seg = document$rows$elements[[1]]$duration$value
seg
seg = document$rows$elements[[1]]
seg
p = read.csv(file.choose())
library(reshape)
install.packages("reshape")
library(reshape)
View(p)
m = melt(p, id = Canton)
m = melt(p, id = Cantón)
p = read.csv(file.choose())
View(p)
m = melt(p, id = Canton)
m = melt(p, id = "Canton")
View(m)
p = read.csv(file.choose())
View(p)
m = melt(p, id = "Canton")
View(m)
names(m) = c("Origen", "Destino", "NumPersonas"
)
View(m)
?gsub
gsub("..",", ",m$Destino)
gsub("..\",", ",m$Destino)
gsub("..\\",", ",m$Destino)
gsub("..",",","Eladio..Montero")
gsub("\..",",","Eladio..Montero")
gsub("\\..",",","Eladio..Montero")
gsub("\\..",", ",m$Destino)
gsub("\\..",", ","San.Jose..San.Jose")
gsub("\\.\\.",", ","San.Jose..San.Jose")
m = gsub("\\.\\.",", ",m$Destino)
m = gsub("\\."," ",m$Destino)
m = gsub("PUNTAREANAS", "PUNTARENAS", m$Destino)
m = melt(p, id = "Canton")
names(m) = c("Origen", "Destino", "NumPersonas")
m$Destino = gsub("\\.\\.",", ",m$Destino)
m$Destino = gsub("\\."," ",m$Destino)
m$Destino = gsub("PUNTAREANAS", "PUNTARENAS", m$Destino)
View(m)
locations = read.xlsx(file.choose())
locations = read.xlsx(file.choose(), sheetIndex = 1)
m[1,1]
m[1,1][[1]]
cantones = read.csv(file.choose(), stringAsFactors = F)
cantones = read.csv(file.choose(), stringsAsFactors = F)
tabla_cantones= melt(cantones, id = "Canton")
names(tabla_cantones) = c("Origen", "Destino", "NumPersonas")
tabla_cantones$Destino = gsub("\\.\\.",", ",tabla_cantones$Destino)
tabla_cantones$Destino = gsub("\\."," ",tabla_cantones$Destino)
tabla_cantones$Destino = gsub("PUNTAREANAS", "PUNTARENAS", tabla_cantones$Destino)
tabla_cantones$Destino = gsub("PUNTAREANAS", "PUNTARENAS", tabla_cantones$Origen)
View(tabla_cantones)
tabla_cantones[1,1]
View(locations)
subset(locations, NOMBRE == tabla_cantones[1,1])
subset(locations, NOMBRE == tabla_cantones[1,1], select = "Location")
data(iris)
summary(iris)
hist(iris$Sepal.Length, iris$Petal.Length)
plot(iris$Sepal.Length, iris$Petal.Length)
plot(iris$Sepal.Length, iris$Petal.Length, color = iris$Species)
?plot
plot(iris)
library(data.table)
# Input data files are available in the "../input/" directory.
# Read in only required columns and force to numeric to ensure that subsequent
# aggregation when calculating medians works
train <- fread('train.csv',
select = c('Agencia_ID', 'Cliente_ID', 'Producto_ID', 'Demanda_uni_equil'),
colClasses=c(Agencia_ID="numeric", Cliente_ID="numeric",Producto_ID="numeric",Demanda_uni_equil="numeric"))
getwd()
setwd("C:\\Users\\eladiomontero\\GrupoBimbo")
train <- fread('train.csv',
select = c('Agencia_ID', 'Cliente_ID', 'Producto_ID', 'Demanda_uni_equil'),
colClasses=c(Agencia_ID="numeric", Cliente_ID="numeric",Producto_ID="numeric",Demanda_uni_equil="numeric"))
train <- fread('DATA\\train.csv',
select = c('Agencia_ID', 'Cliente_ID', 'Producto_ID', 'Demanda_uni_equil'),
colClasses=c(Agencia_ID="numeric", Cliente_ID="numeric",Producto_ID="numeric",Demanda_uni_equil="numeric"))
products <- fread('DATA\\cluster_prods(20).csv',
select = c('Producto_ID',    'Cluster'),
colClasses = c(Producto_ID="numeric", Cluster="character"))
products <- fread('DATA\\cluster_prods(20).csv',
select = c('Producto_ID',    'Cluster'),
colClasses = c(Producto_ID="numeric", Cluster="character"), mode='wb')
products <- fread('DATA\\cluster_prods(20).csv',
select = c('Producto_ID',    'Cluster'),
colClasses = c(Producto_ID="numeric", Cluster="character"))
products <- fread('DATA\\cluster_prods(20).csv',
select = c('Producto_ID',    'Cluster'),
colClasses = c(Producto_ID="numeric", Cluster="character"))
products <- fread('DATA\\cluster_prods(20).csv',
select = c('Producto_ID',    'Cluster'),
colClasses = c(Producto_ID="numeric", Cluster="character"))
train2 <- merge(train, products, by = "Producto_ID")
train = train[1:1000000,]
train2 <- merge(train, products, by = "Producto_ID")
View(products)
View(train)
train2 <- merge(products, train, by = "Producto_ID")
train2 <- merge(train, products, by = "Producto_ID")
allow.cartesian=TRUE
train2 <- merge(train, products, by = "Producto_ID")
train2 <- merge(train, products, by = "Producto_ID", allow.cartesian=TRUE)
View(train2)
train2 = merge(train, products, by = train$Producto_ID)
train2 = merge(train, products, by = "Producto_ID")
View(products)
train2 = merge(train, products, by.x = "Producto_ID")
train2 = merge(train, products)
train2 = merge(train, products, by.x = "Producto_ID", all.x = TRUE)
train2 = merge(train, products, by = "Producto_ID", all.x = TRUE)
train2 = merge(train, products, by = "Producto_ID", all.x = TRUE, all.y = FALSE)
train2 = merge(train, products)
names(train)
names(products)
train2 = merge(train, products, by = "Producto_ID", all = TRUE)
train2 = merge(train, products, by.x = "Producto_ID", by.y = "Producto_ID")
summary(train)
summary(produts)
summary(products)
setkey(train, Producto_ID, Agencia_ID, Cliente_ID)
setkey(products, Producto_ID)
train2 <- merge(train, products, by = "Producto_ID")
train2 <- base::merge(train, products, by = "Producto_ID")
train2 <- base::merge(train, products, by = "Producto_ID", all = FALSE)
length(products$Producto_ID)
length(unique(products$Producto_ID))
save.image("C:/Users/eladiomontero/GrupoBimbo/session.RData")
